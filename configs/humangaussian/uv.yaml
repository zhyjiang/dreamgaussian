### Output
outdir: checkpoints 
mesh_format: obj

save_path: /work/u1436961/ZhongyuJ/checkpoints/proj_scale_factor_0.2_2
save_name: /work/u1436961/ZhongyuJ/checkpoints/proj_scale_factor_0.2_2 #similarity score feature level
vis_path: /work/u1436961/ZhongyuJ/checkpoints/proj_scale_factor_0.2_2
vis_ply_path: /work/u1436961/ZhongyuJ/checkpoints/proj_scale_factor_0.2_2
vis_novel_view: /work/u1436961/ZhongyuJ/checkpoints/proj_scale_factor_0.2_2
save_interval: 50 
eval_interval: 4
eval_only: False # only if evaluation 
exp_scale: False # if you want torch.exp as scale's activation 
clip_scaling: 0.02 # clip maximum for exp_scale
smpl_driven_training: False #train with unpaired smpl/image
fake_ratio: 0.0 #ratio of the unpaired data
scale_loss: True # the loss to constratin scale to make it more sphere-wise

# pretrain_path: /home/u1436961/Zhongyuj/dreamgaussian/proj_scale_exp_2/work/u1436961/ZhongyuJ/checkpoints/proj_scale_exp_2/epoch450.pth
pretrain_path: /home/u1436961/Zhongyuj/dreamgaussian/proj_scale_factor_0.2/work/u1436961/ZhongyuJ/checkpoints/proj_scale_factor_0.2/epoch100.pth

input_mode: 'image+smpl' # 'image+smpl' or 'uv' or 'shape'
train_strategy:  'both' #'uv' and 'shape' or 'both' for joint training
learned_scale : False 
learned_scale_weight: 0.05
superresolution: False 
switch_epoch : 4 #2 # this is only used in gan-wise traininig, skipped for joint learning


dino_interpolation: False # dino_interpolation for original resolution processing 
uv_query: True 
learned_w: False
return_RT: False
k: 5
pretrain_uv: False # used in gan-wise training, ignored in joint training



crop_image: False #crop out the human image
train_from_scratch: False #dino for train_from_scratch 
dino:
  image_embed: True
  name: ViT-B
  path: dino_vitb16


reshapeto224: True
vis_depth_path: temp_depth_ZJU_temp_lower_scale
vis_eval: temp_vis_eval_ZJU_temp_lower_scale
vis_eval_depth_path: temp_depth_eval_ZJU_temp_lower_scale
dataset_type:  ZJU
dataset:
  name: ZJU  #HuMMan
  # path: [./data/ZJUMOCAP/CoreView_313, ./data/ZJUMOCAP/CoreView_314, ./data/ZJUMOCAP/CoreView_315, ./data/ZJUMOCAP/CoreView_316, ./data/ZJUMOCAP/CoreView_317, ./data/ZJUMOCAP/CoreView_318, ./data/ZJUMOCAP/CoreView_319, ./data/ZJUMOCAP/CoreView_320, ./data/ZJUMOCAP/CoreView_321, ./data/ZJUMOCAP/CoreView_322, ./data/ZJUMOCAP/CoreView_323, ./data/ZJUMOCAP/CoreView_324, ./data/ZJUMOCAP/CoreView_325, ./data/ZJUMOCAP/CoreView_326, ./data/ZJUMOCAP/CoreView_327, ./data/ZJUMOCAP/CoreView_328, ./data/ZJUMOCAP/CoreView_329, ./data/ZJUMOCAP/CoreView_330, ./data/ZJUMOCAP/CoreView_331, ./data/ZJUMOCAP/CoreView_332, ./data/ZJUMOCAP/CoreView_333, ./data/ZJUMOCAP/CoreView_334, ./data/ZJUMOCAP/CoreView_335, ./data/ZJUMOCAP/CoreView_336, ./data/ZJUMOCAP/CoreView_337, ./data/ZJUMOCAP/CoreView_338, ./data/ZJUMOCAP/CoreView_339, ./data/ZJUMOCAP/CoreView_340, ./data/ZJUMOCAP/CoreView_341, ./data/ZJUMOCAP/CoreView_342, ./data/ZJUMOCAP/CoreView_343, ./data/ZJUMOCAP/CoreView_344, ./data/ZJUMOCAP/CoreView_345, ./data/ZJUMOCAP/CoreView_346, ./data/ZJUMOCAP/CoreView_347, ./data/ZJUMOCAP/CoreView_348, ./data/ZJUMOCAP/CoreView_349, ./data/ZJUMOCAP/CoreView_350, ./data/ZJUMOCAP/CoreView_351, ./data/ZJUMOCAP/CoreView_352, ./data/ZJUMOCAP/CoreView_353, ./data/ZJUMOCAP/CoreView_354, ./data/ZJUMOCAP/Core
  root: ./data/ZJUMOCAP/ 

  path: [386]
  test_path: [386]
  camera_list: [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23]
  train_camera_list : [1,3,5,7,9,11,13,15,17,19,21,23] 
  test_camera_list : [2,4,6,8,10,12,14,16,18,20,22]
  num_workers: 4
  smpl_coord: True
  sample_rate: 5
  eval_sample_rate: 200
  img_emb_dim:  4096
  H: 1024 #224 #1024
  W: 1024 #224 #1024
  pose_num: 24
  img_channel: 196

dataset2:
  name: HuMMan
  smpl_path : SMPL/ROMP_SMPL/SMPL_NEUTRAL.pth
  root: /mnt/nvme/Dataset/HuMMan/
  split: train
  multi_person: True
  num_workers: 0
  num_instances: 1
  img_emb_dim:  196 #8040
  H: 1080
  W: 1920
  pose_num: 23

output_points: 6890

### Training
# guidance loss weights (0 to disable)
canonical: False
param_input: True
camera_param : False
trans_decoder: False
cross_attn: True
lambda_sd: 1
mvdream: False
lambda_zero123: 0
full_token: True
reshape: True
dino_update : True #fine tune dino
# training batch size per iter
batch_size: 1
# training iterations for stage 1
iters: 500

multi_view: 1
# whether to linearly anneal timestep
anneal_timestep: True
# training iterations for stage 2
iters_refine: 50
# training camera radius
radius: 2.5
# training camera fovy
fovy: 49.1
# checkpoint to load for stage 1 (should be a ply file)
load:
# whether allow geom training in stage 2
train_geo: False
# prob to invert background color during training (0 = always black, 1 = always white)
invert_bg_prob: 0.5


means3D_scale: 0.01
similarity_score_weight: 0.1
ssim_weight: 0.02
scale_factor: 0.2 #0.001  #tunable very tricky
smpl_reg_scale: 0.01 #0.01 
mask_weight: 0.1
scale_constrain_weight: 2.0


triple_point: False
upsample: 1 # 1
loss:  ssim_l2 # [l1 l2 ssim_l2 ssim_l1] #silou loss needs to be added in the next version
reg_loss: l2 # [l1 l2 or None]
late_can2w: False
triplane : False
lr: 0.001 #0.001
base_lr: 0.0001 #0.0001
warmup: 10 #10

### GUI
gui: False
force_cuda_rast: False
# GUI resolution
# H: 1024

# W: 1024
near: 0.001
far: 100

### Gaussian splatting
num_pts: 5000
sh_degree: 0
position_lr_init: 0.001
position_lr_final: 0.00002
position_lr_delay_mult: 0.02
position_lr_max_steps: 300
feature_lr: 0.01
opacity_lr: 0.05
scaling_lr: 0.005
rotation_lr: 0.005
percent_dense: 0.01
density_start_iter: 1
density_end_iter: 3000
densification_interval: 50
opacity_reset_interval: 700
densify_grad_threshold: 0.01

### Textured Mesh
geom_lr: 0.0001
texture_lr: 0.2